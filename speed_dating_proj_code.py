# -*- coding: utf-8 -*-
"""Speed_Dating_Proj_feature_eng_new.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1afiiFAvIpt5edYP4iQjFyrqN8h4B_xs_
"""

from google.colab import drive
drive.mount('/content/drive')

# import packages
import pandas as pd
import matplotlib.pyplot as plt
from pylab import rcParams
import numpy as np
from sklearn.model_selection import train_test_split

# read files
org_df = pd.read_csv('/content/drive/MyDrive/dsci531/project_dsci531/data/Speed_Dating_Data.csv')

org_df

org_df[['iid', 'gender','wave', 'match', 'race','attr1_1','sinc1_1','intel1_1', 'amb1_1','attr1_1', 'fun1_1']]

# print columns to see
for i in org_df.columns:
    print(i)

"""# I)_**EDA**_"""

# unique iid , each iid = each person
# res: draw stacked Percentage of races in Man vs in Woman
# find race for participant
# pid_race = org_df[['pid','race_o']]
# org_df['race_p'] = org_df.merge(pid_race, left_on='iid', right_on = 'pid', how='left')['race_o_y']
race_freq_male = []
race_freq_female = []
races = []
# find freq(ie number of male and female) of each race
for race in org_df['race'].unique():
    races.append(race)
    # sort races to improve visualization
races_sort = [races[1], races[0], races[3], races[2], races[4]]
for race in races_sort:
    race_freq_male.append(len(org_df[(org_df['race'] == race) & (org_df['gender'] == 1)].iid.unique()))
    race_freq_female.append(len(org_df[(org_df['race'] == race) & (org_df['gender'] == 0)].iid.unique()))

races_str = [str(a) for a in races_sort]
plt.bar(races_str, race_freq_male, color='r')
plt.bar(races_str, race_freq_female, bottom=race_freq_male, color='b')
plt.title("Frequency of male and female in each race")
plt.xlabel("race")
plt.ylabel("Frequency")
plt.legend(['Male',"Female"])

"""## Takeaway
#### 1. races are not equally distributed in our dataset.
#### 2. Male and female are equally distributed in each race
"""

# Draw a pie chart for matches vs not matched
matches_no_matches = [len(org_df[(org_df['match'] == 0)]), len(org_df[(org_df['match'] == 1)])]
Match_name = ['No_match', 'Match']
plt.pie(matches_no_matches, labels = Match_name )
plt.title("Percentage of Match VS. Not Match")
plt.show()

"""# Takeaway
#### 1. Imbalanced data
"""

# draw stacked bar chart of frequency of male and female in each age
# similarly, we should find ages for participant first
# pid_age = org_df[['pid','age_o']]
# org_df['age_p'] = org_df.merge(pid_age, left_on='iid', right_on = 'pid', how='left')['age_o_y']
age_freq_male = []
age_freq_female = []
ages = []
# find freq(ie number of male and female) of each race
for age in org_df['age'].unique():
    ages.append(age)

for age in ages:
    age_freq_male.append(len(org_df[(org_df['age'] == age) & (org_df['gender'] == 1)].iid.unique()))
    age_freq_female.append(len(org_df[(org_df['age'] == age) & (org_df['gender'] == 0)].iid.unique()))

plt.bar(ages, age_freq_male, color='r')
plt.bar(ages, age_freq_female, bottom=age_freq_male, color='b')
plt.title("Frequency of male and female in each age")
plt.xlabel("Age")
plt.ylabel("Frequency")
plt.legend(['Male',"Female"])

"""# Takeaway
#### 1. Most of our participants are aged between 21 and 30. In other words, they are young adults.
#### 2. In general, number of male equals to number of female at each age
"""

# for each wave, number of male vs number of female
wave_freq_male = []
wave_freq_female = []
waves = org_df.wave.unique()

# sort values for better visualizations
wave_freq_dic = {} # create a dictionary to see sorted order of wave (order from highest freq to lowest)
for wave in waves:
    wave_freq_dic[wave] = [len(org_df[(org_df['wave'] == wave)].iid.unique()),  len(org_df[(org_df['wave'] == wave) & (org_df['gender'] == 1)].iid.unique()), len(org_df[(org_df['wave'] == wave) & (org_df['gender'] == 0)].iid.unique())]
sort_dic = {k: v for k, v in sorted(wave_freq_dic.items(), key=lambda item: item[1][0])}
waves = list(sort_dic.keys())
wave_freq_male = [a[1] for a in sort_dic.values()]
wave_freq_female = [a[2] for a in sort_dic.values()]




waves_str = [str(a) for a in waves]
plt.bar(waves_str, wave_freq_male, color='r')
plt.bar(waves_str, wave_freq_female, bottom=wave_freq_male, color='b')
plt.title("Frequency of male and female in each wave")
plt.xlabel("Wave")
plt.ylabel("Frequency")
plt.legend(['Male',"Female"])

"""# Takeaway
#### 1. The number of female and the number of male are equal for all waves.
#### 2. Number of participants are different in different waves.
#### 3. Number of participants is smallest in wave 6 and biggest in wave 21
#### 4. There are 20 - 50 number of participants in most of waves.

"""

# frequency of decision of partner the night of event
decisions = org_df.dec_o.unique()
dec_freq_dic ={}
for decision in decisions:
    dec_freq_dic[decision] = len(org_df[org_df['dec_o'] == decision])

dec_sort_dic = {k: v for k, v in sorted(dec_freq_dic.items(), key=lambda item: item[1])}
decisions = list(dec_sort_dic.keys())
dec_freq = [a for a in dec_sort_dic.values()]

dec_str = [str(a) for a in decisions]
plt.bar(dec_str, dec_freq)
plt.title("Frequency of decisions")
plt.xlabel("Decision")
plt.ylabel("Frequency")
plt.show()

"""# Takeaway
#### Most of first night decision from partner is no
"""

org_df.attr_o.unique()

# draw bar plot showing number of genders in each field
# for each field, number of male vs number of female
field_freq_male = []
field_freq_female = []
fields = org_df.field_cd.unique()

# sort values for better visualizations
field_freq_dic = {} # create a dictionary to see sorted order of wave (order from highest freq to lowest)
for field in fields:
    field_freq_dic[field] = [len(org_df[(org_df['field_cd'] == field)].iid.unique()),  len(org_df[(org_df['field_cd'] == field) & (org_df['gender'] == 1)].iid.unique()), len(org_df[(org_df['field_cd'] == field) & (org_df['gender'] == 0)].iid.unique())]
sort_dic = {k: v for k, v in sorted(field_freq_dic.items(), key=lambda item: item[1][0])}
fields = list(sort_dic.keys())
field_freq_male = [a[1] for a in sort_dic.values()]
field_freq_female = [a[2] for a in sort_dic.values()]




field_str = [str(int(a)) if a == a else "NaN" for a in fields ]
plt.bar(field_str, field_freq_male, color='r')
plt.bar(field_str, field_freq_female, bottom=field_freq_male, color='b')
plt.title("Frequency of male and female in each field")
plt.xlabel("Field")
plt.ylabel("Frequency")
plt.legend(['Male',"Female"])
plt.figure(figsize=(1,40))
plt.show()

"""# Takeaway
#### 1. Number of female and number of male are far away from each other in all field
#### 2. Male are dominated in most of fields among our participants
#### 3. Female are dominated in field 3, 9, 11, 6, 15, 4 which are Social Science, Psychologist, Education, Academia, Social Work, English/Creative Writing/ Journalism, Fine Arts/Arts Administration, Medical Science, Pharmaceuticals, and Bio Tech. What interested me is that these fields non-STEM fields and others are STEM fields. Thus, this graph might imply the bias in the enrollment for Columbia university.

"""

# find statistics od median score in underg school
# change df to participant level to see the distribution
people_level_income = org_df.groupby(['iid', 'income'], group_keys=False).apply(lambda x: x)
SATs = people_level_income.mn_sat
x = pd.Series([float(str(a).replace(",",'')) for a in SATs if a == a])
x.describe()

"""# Takeaway
#### 1. The Median SAT score for the undergraduate institution where attended are between 914 to 1490.
#### 2. Median of scores is 1310 which is high in 2016, so most of participants are intelligent.
"""

# find missing values
org_df.isna().sum().reset_index(name="n", drop = True).plot.bar(x='index', y='n', rot = 45)

len(org_df)

"""# Takeaway
#### Most of columns have more than 2000 missing values but we only have 8378 rows in total, so we defintely need to preprocess the missing values before modeling.
"""

# see the freq of zip codes (zipcode bias)
zipcode_peo_level = org_df.groupby(['iid', 'zipcode'], group_keys=False).apply(lambda x: x)
zips = zipcode_peo_level.zipcode.unique()
zip_freq_dic ={}
for zipcode in zips:
    zip_freq_dic[zipcode] = len(zipcode_peo_level[zipcode_peo_level['zipcode'] == zipcode].iid.unique())

zip_sort_dic = {k: v for k, v in sorted(zip_freq_dic.items(), key=lambda item: item[1])}
zips = list(zip_sort_dic.keys())
zip_freq = [a for a in zip_sort_dic.values()][390:]

zip_str = [str(a) for a in zips][390:]
plt.bar(zip_str, zip_freq)
plt.title("Frequency of Zipcode")
plt.xlabel("Zipcode")
plt.ylabel("Frequency")
rcParams['figure.figsize'] = 10, 5
plt.show()

zip_freq = [a for a in zip_sort_dic.values()][400:]

zip_str = [str(a) for a in zips][400:]
plt.bar(zip_str, zip_freq)
plt.title("Frequency of Zipcode")
plt.xlabel("Zipcode")
plt.ylabel("Frequency")
from pylab import rcParams
rcParams['figure.figsize'] = 10, 5
plt.show()

"""# Takeaway
#### 1. Most of participants grew up in the east of America( New York)
#### 2. There are a large portion of missing values in Zipcode
"""

# income statistics
people_level_inc = org_df.groupby(['iid', 'income'], group_keys=False).apply(lambda x: x)
incomes = people_level_inc.income
x = pd.Series([float(str(a).replace(",",'')) for a in incomes if a == a]) # drop na
x.describe()

# see the income distribution
incomes = org_df.income.unique()
income_freq_dic ={}
for income in incomes:
    income_freq_dic[income] = len(org_df[org_df['income'] == income].iid.unique())

income_sort_dic = {k: v for k, v in sorted(income_freq_dic.items(), key=lambda item: item[1])}
incomes = list(income_sort_dic.keys())
income_freq = [a for a in income_sort_dic.values()]

income_str = [str(a) for a in incomes]
plt.bar(zip_str, zip_freq)
plt.title("Frequency of Income")
plt.xlabel("Income")
plt.ylabel("Frequency")
rcParams['figure.figsize'] = 10, 5
plt.show()

"""# Takeaway
#### 1. 0 means missing values, so there are a lot of missing values in income
#### 2. U.S. Median Household Income is 57,617 dollars in 2016, so all participants come from median to high level economic area.

#### Find relationship between males' intelligence(mn_sat) and famale's intelligence.
"""

def change_obj_num(row, col):
    if row[col] == row[col]:
        if row[col] != []:
            if ',' in str(row[col]):
                number = float(row[col].replace(',', ''))
            else:
                number = float(row[col])
        else:
            number = np.nan
        return number
    else:
        return np.nan

# first find mn_sat for partners
pid_sat = org_df[org_df['gender'] == 0].groupby(['iid'])['mn_sat'].agg(pd.Series.mode)
# shrink matches with only gender 1(male)
male_df = org_df[org_df['gender'] == 1]
male_df['mn_sat_o'] = male_df.merge(pid_sat, left_on='pid', right_on = 'iid', how='left')['mn_sat_y']
# change sat to number
male_df['mn_sat_o'] = male_df.apply(lambda row: change_obj_num(row, 'mn_sat_o'), axis = 1)
male_df['mn_sat'] = male_df.apply(lambda row: change_obj_num(row, 'mn_sat'), axis = 1)

len(male_df['mn_sat_o']) == len(male_df.merge(pid_sat, left_on='pid', right_on = 'iid', how='left')['mn_sat_y'])

succ_male_df = male_df[male_df['match'] == 1]

succ_male_df = succ_male_df[(succ_male_df['mn_sat'] == succ_male_df['mn_sat']) & (succ_male_df['mn_sat_o'] == succ_male_df['mn_sat_o'])]
#create basic scatterplot
plt.plot(succ_male_df['mn_sat'], succ_male_df['mn_sat_o'], 'o')

#obtain m (slope) and b(intercept) of linear regression line
m, b = np.polyfit(succ_male_df['mn_sat'], succ_male_df['mn_sat_o'],1)

#add linear regression line to scatterplot
plt.plot(succ_male_df['mn_sat'], m*succ_male_df['mn_sat']+b)
plt.title('Relationship Between Male Intelligence(mn_sat) And Famale Intelligence for Successful Match')
plt.xlabel('Male mn_sat')
plt.ylabel('Female(partner) mn_sat')

succ_male_df = male_df[male_df['match'] == 1]

succ_male_df = succ_male_df[(succ_male_df['attr'] == succ_male_df['attr']) & (succ_male_df['attr_o'] == succ_male_df['attr_o'])]
#create basic scatterplot
plt.plot(succ_male_df['attr'], succ_male_df['attr_o'], 'o')

#obtain m (slope) and b(intercept) of linear regression line
m, b = np.polyfit(succ_male_df['attr'], succ_male_df['attr_o'],1)

#add linear regression line to scatterplot
plt.plot(succ_male_df['attr'], m*succ_male_df['attr']+b)
plt.title('Relationship Between Male Attractiveness And Famale Attractiveness for Successful Match')
plt.xlabel('Male Attractiveness')
plt.ylabel('Female(partner) Attractiveness')

"""# II) _Preprocessing_

### Filter columns

#### Since all attributes gained after time 2 survey are scores/feelings gained after deciding match or not, we should not use these columns to predict a person's match rate. Thus, we need to filter our these attributes for our prediction
"""

# Filter columns which are useful for predicting match decisions
# find list of column names that should be kept
column_names_org = list(org_df.columns)
index_last_col = column_names_org.index('amb3_s')
filtered_df = org_df[column_names_org[:index_last_col+1]]

"""#### Attributes such as career and field are repetitive for coded columns career_c, field_c so we can delete them"""

filtered_df.drop(['field', 'career'], axis = 1, inplace=True)

"""#### Similarly,  we can distinguish a partner based on pid so we don't need the column partner: 	partner’s id number"""

filtered_df.drop(['partner'], axis = 1, inplace=True)

"""#### Similarly,  we can distinguish a participant based on iid so we don't need the column id"""

filtered_df.drop(['id','idg'], axis = 1, inplace=True)

"""#### station number and station number where met partner cannot be controlled by a participant/his partner , so we don't explore relationship between these columns and match rate to analyze matching rate. Additionally, these two attributes would not exist in real speed dating app. Thus, we ignore these columns for match rate prediction."""

filtered_df.drop(['position', 'positin1', 'condtn'], axis = 1, inplace=True)

"""#### Since undergraduate school level attended can be representated by Median SAT score for the undergraduate institution where attended, we ignore undergraduate school column in our analysis."""

filtered_df.drop(['undergra'], axis = 1, inplace=True)

"""#### From EDA for zipcode, we can clearly see that most of participants comes from eastern U.S., so it won't affect our prediction and we don't need to include the column with same value in our analysis."""

filtered_df.drop(['from', 'zipcode'], axis = 1, inplace=True) # no such col

"""#### dec_o, dec is the decision of match/not match by the partner which is strong correlated with final decision which is resulted by decisions from the participant and the partner, so we delete this column."""

filtered_df.drop(['dec_o', 'dec'], axis = 1, inplace=True)

"""#### met and met_o should only contains 0 or 1 for have not met or met before, but it has various values. There is no explaination for the various values, so we delete the col to avoid any possible bias."""

filtered_df['met'].unique()
filtered_df.drop(['met_o','met'], axis = 1, inplace=True)

"""##### delete col which have 70% missing values"""

for c in filtered_df.columns:
    if filtered_df[c].isna().sum()/len(filtered_df) > 0.7:
        filtered_df.drop(c, axis = 1, inplace = True)

"""### Imputation

#### We fill missing values related to each participant with information entered by the participant.
"""

/# check if iid has no missing value to conduct the process
filtered_df.iid.isna().sum()

# check if pid has no missing value to conduct the process
filtered_df.pid.isna().sum()

# exploration of why pid has missing values
a = org_df[org_df['pid'] != org_df['pid']]#['partner']
a
# wave 5 has 10 men, and there are different 10 men who has nan pid for their partner, so the nan pid should be the same female.
print(len(org_df[org_df['wave'] == 5]))
# only 190 matches found but there should be 200 matches if every female and male show up. so there's possibility that the woman doesn't show up
print(org_df[org_df['wave'] == 5].pid.unique())
print("missing female pid is: 118")
# check info in iid = pid = 118
org_df[org_df['iid'] == 118]
# iid = 118 has no info, so the female with pid 118 has participated in our experiment
# Thus, we should delete row corresponding to her in our dataset which is the rows with missing pid
filtered_df = filtered_df[filtered_df['pid'] == filtered_df['pid']]

"""#### Before imputation, we need to normalize the format of NaN values. For example 0 in income is actually missing values."""

# col value 0 is missing value
reasonable0 = ['gender', 'match', 'int_corr', 'samerace', 'dec', 'met', 'match_es' ]
column0 = [a for a in filtered_df.columns if (a not in reasonable0)]
for col in column0:
  filtered_df[col] = filtered_df[col].replace([0,'0'], np.nan)
for c in filtered_df.columns:
  filtered_df[c] = filtered_df[c].replace(['nan','NaN'], np.nan)

filtered_df.isna().sum().reset_index(name="n", drop = True).plot.bar(x='index', y='n', rot = 45)

##### delete col which have 70% missing values
for c in filtered_df.columns:
    if filtered_df[c].isna().sum()/len(filtered_df) > 0.7:
        filtered_df.drop(c, axis = 1, inplace = True)

"""##### for columns gender, wave, round, age, field_cd, mn_sat, tuition, race, imprace, imprelig, income, goal, date, go_out, career_c, interests,  exphappy, expnum.... stays the same whoever the participant met with, so we can impute missing values with modes for each column entered by the participant for other dates. On the other hand, there are some columns, such as pf_o_att, age_o, race_o , which should mainly remain the same for the partner, so we impute missing values with modes for each column entered by the partner for other dates.

"""

# while we should impute after train test split, since this imputation method will work the same
# with or without train test split
# to save time, we impute without train test spliting
columns_for_imp_p = ['gender', 'wave', 'round', 'order'] + list(filtered_df.columns[26:85]) +  list(filtered_df.columns[95:])
for column in columns_for_imp_p:
    if filtered_df[column].isna().sum() != 0:
        #print(column)
        try:
            filtered_df[column] = filtered_df[column].fillna(filtered_df.groupby('iid')[column].agg(pd.Series.mode))
        except ValueError:
            ###########################################################
            filtered_df[column] = filtered_df[column].fillna(filtered_df[column].agg(pd.Series.mode)) # find a way to fix

filtered_df.isna().sum().reset_index(name="n", drop = True).plot.bar(x='index', y='n', rot = 45)
### not much changes

"""#### Since people in different waves have different score ranges, we need to standardize them"""

wave6_9 = filtered_df[(filtered_df['wave'] == 6) | (filtered_df['wave'] == 7) | (filtered_df['wave'] == 8) | (filtered_df['wave'] == 9)]
other_waves = filtered_df[(filtered_df['wave'] != 6) & (filtered_df['wave'] != 7) & (filtered_df['wave'] != 8) & (filtered_df['wave'] != 9)]
columns_with_ = [a for a in filtered_df.columns if "_" in a]
scores_col_for_wave6_9 = columns_with_[3:9] + columns_with_[21:49]
for score_col in scores_col_for_wave6_9:
    wave6_9[score_col] = wave6_9.apply (lambda row: (row[score_col] * 100)/60, axis=1)
# combine waves together
waves = [other_waves, wave6_9]
stand_df = pd.concat(waves)

"""#### There are definitely people who didn't fill anything in some column from attributes above, we used medians for numeric and modes for categorical in each column(KNN imputation methods if have time)

#### Before modeling(KNN)/finding appropriate values to fill in, we need to change all columns into numeric
"""

# change income type can use function below too
def change_income_type(row):
    if row['income'] == row['income']:
        num_income = float(row['income'].replace(',', ''))
        return num_income
    else:
        return np.nan

# change object to float
def change_obj_num(row, col):
    if row[col] == row[col]:
        if row[col] != []:
            if ',' in str(row[col]):
                number = float(row[col].replace(',', ''))
            else:
                number = float(row[col])
        else:
            number = np.nan
        return number
    else:
        return np.nan

# income, tuition, expnum, mn_sat ,sinc1_1, amb1_1, shar1_1, sinc2_1, intel2_1, amb2_1, shar2_1 are not numeric
len([a for a in stand_df.dtypes if a!= 'float64' and a!= 'int64']) # check non numeric cols
# find reason and solution why non numeric
stand_df['income'] = stand_df.apply (lambda row: change_income_type(row), axis=1)
# expnum has more than 70% missing, so we delete tuition col
# stand_df.expnum.isna().sum()/len(stand_df)
# similarly, we drop expnum
# stand_df.drop('expnum', axis = 1, inplace = True)


stand_df['sinc1_1'] = stand_df.apply (lambda row: change_obj_num(row, 'sinc1_1'), axis=1)
stand_df['amb1_1'] = stand_df.apply (lambda row: change_obj_num(row, 'amb1_1'), axis=1)
stand_df['shar1_1'] = stand_df.apply (lambda row: change_obj_num(row, 'shar1_1'), axis=1)
stand_df['sinc2_1'] = stand_df.apply (lambda row: change_obj_num(row, 'sinc2_1'), axis=1)
stand_df['intel2_1'] = stand_df.apply (lambda row: change_obj_num(row, 'intel2_1'), axis=1)
stand_df['amb2_1'] = stand_df.apply (lambda row: change_obj_num(row, 'amb2_1'), axis=1)
stand_df['shar2_1'] = stand_df.apply (lambda row: change_obj_num(row, 'shar2_1'), axis=1)
stand_df['mn_sat']= stand_df.apply (lambda row: change_obj_num(row, 'mn_sat'), axis=1)
stand_df['tuition']= stand_df.apply (lambda row: change_obj_num(row, 'tuition'), axis=1)

# for the problem existed in train_df[num].fillna(value= train_df[num].median(), inplace=True) below
stand_df['sinc'] = stand_df['sinc'].apply(lambda x: np.mean(x) if type(x) != float else x)

len([a for a in stand_df.dtypes if a!= 'float64' and a!= 'int64'])
# it shows we changed all types to float now

"""##### After changing real categorical (such as career_c) to numeric, we need to make sure all multicategorical values are encoded and ready for KNN.

#### KNN imputation
"""

from sklearn.impute import KNNImputer
# multiclass categorical variables include  wave, field_cd, race, goal, date, go_out, career_c,
# In order to get one hot encoding of columns in the future, we shoud change their col type to strings
mul_cat = ['wave', 'field_cd', 'race', 'goal', 'date', 'go_out', 'career_c']
for c in mul_cat:
    stand_df[c] = stand_df[c].astype(str)
bi_class = ['gender'] # gender is for prediction of match rate so we don't need to change it now
categorical_var = mul_cat + bi_class
# train test split before imputation
unique_iid = stand_df.iid.unique()
train, test = train_test_split(unique_iid, test_size=0.33, random_state=42)
train_df = stand_df[stand_df['iid'].isin(train)]
test_df = stand_df[stand_df['iid'].isin(test)]
imputer = KNNImputer(n_neighbors=2, weights="uniform")
train_imp = imputer.fit_transform(train_df)
test_imp = imputer.transform(test_df)
train_imp_df = pd.DataFrame(train_imp, columns = stand_df.columns)
test_imp_df = pd.DataFrame(test_imp, columns = stand_df.columns)
imputed_df = pd.concat([train_imp_df, test_imp_df])

"""#### Gaussian Imputaion"""

stand_df

# multiclass categorical variables include  wave, field_cd, race, goal, date, go_out, career_c,
# In order to get one hot encoding of columns in the future, we shoud change their col type to strings
mul_cat = ['wave', 'field_cd', 'race', 'goal', 'date', 'go_out', 'career_c']
for c in mul_cat:
    stand_df[c] = stand_df[c].astype(str)
bi_class = ['gender'] # gender is for prediction of match rate so we don't need to change it now
categorical_var = mul_cat + bi_class
# train test split before imputation
unique_iid = stand_df.iid.unique()
train, test = train_test_split(unique_iid, test_size=0.33, random_state=42)
train_df = stand_df[stand_df['iid'].isin(train)].to_numpy()[:,:-1]
test_df = stand_df[stand_df['iid'].isin(test)].to_numpy()[:,:-1]
from gcimpute.gaussian_copula import GaussianCopula
from gcimpute.helper_data import generate_mixed_from_gc
from gcimpute.helper_evaluation import get_smae
from gcimpute.helper_mask import mask_MCAR
import numpy as np
'''
# generate and mask 15-dim mixed data (5 continuous variables, 5 ordinal variables (1-5) and 5 boolean variables)

X_mask = mask_MCAR(X = stand_df, mask_fraction=0.1, seed=101)
model = GaussianCopula(verbose=1)
imputed_df = model.transform(X_mask)

# model fitting
X_mask = mask_MCAR(X = train_df, mask_fraction=0.1, seed=101)
model = GaussianCopula(verbose=1)
X_train_imp = model.fit_transform(X=X_mask)
X_test_imp = model.transform(test_df)
# combine train test for feature engineering
# imputed_df = pd.concat([X_train_imp, X_test_imp])
'''

X_mask = mask_MCAR(X = train_df, mask_fraction=0.1, seed=101)

"""#### Median Imputation

##### we use medians for numeric and modes for categorical in each column(KNN imputation methods if have time)
"""

# multiclass categorical variables include  wave, field_cd, race, goal, date, go_out, career_c,
# In order to get one hot encoding of columns in the future, we shoud change their col type to strings
mul_cat = ['wave', 'field_cd', 'race', 'goal', 'date', 'go_out', 'career_c']
for c in mul_cat:
    stand_df[c] = stand_df[c].astype(str)
bi_class = ['gender'] # gender is for prediction of match rate so we don't need to change it now
categorical_var = mul_cat + bi_class
# train test split before imputation
unique_iid = stand_df.iid.unique()
train, test = train_test_split(unique_iid, test_size=0.33, random_state=42)
train_df = stand_df[stand_df['iid'].isin(train)]
test_df = stand_df[stand_df['iid'].isin(test)]
# fill na with mode/medians
for cat in categorical_var:
    train_df[cat] = train_df[cat].fillna(train_df[cat].agg(pd.Series.mode).squeeze())
    test_df[cat] = test_df[cat].fillna(train_df[cat].agg(pd.Series.mode).squeeze())
num_var = list(set(list(stand_df.columns)) - set(categorical_var))
for num in num_var:
    train_df[num].fillna(value= train_df[num].median(), inplace=True)
    test_df[num].fillna(value= train_df[num].median(), inplace=True)

# combine train test for feature engineering
imputed_df = pd.concat([train_df, test_df])


'''one_hot = pd.get_dummies(stand_df[mul_cat])
# Drop column B as it is now encoded
stand_df = stand_df.drop(mul_cat,axis = 1)
# Join the encoded df
stand_df = stand_df.join(one_hot)'''

imputed_df = pd.concat([train_df, test_df])

imputed_df.isna().sum().reset_index(name="n", drop = True).plot.bar(x='index', y='n', rot = 45)
### no nan

"""# III) _Feature Engineering_

---
#### Drop unncessary columns

Some variables only represents the group index and partner order in the experiment, which are not useful in the participant level dataframe. We only care about the personal characteristics of the participant. So we drop the following variables from our imputated dataframe:
- `wave`, `round`, `order`, `pid`,

For tuition column, it has the same distribution as the `mn_sat` column, so we just keep the `mn_sat` column and drop `tuition`
- `tuition`

For the match/decision features, seperate them out to be a individual dataframe
- `match`, `dec`
---
"""

group_info_col = ['wave', 'round', 'order', 'pid']
useless_col = ['tuition']
# records the match results for each participant with its partenr information
match_result_df = imputed_df[['iid', 'pid', 'match']]

# drop unnecessary features
imputed_df = imputed_df.drop(columns = group_info_col + useless_col + ['match'])

"""---
### One-hot encode the categorical columns
- convert the categorical codes to more meaningful column names and for each participant, extract their categorical variables to be the participant level information
- the code dictionary stores what each one-hot code represents
- for the participant who does not answer the question, label them as the category `cte_-1`, which is Not Answered
- Those categorical columns are the following:
    - `field_cd`, `race`, `race_o`,`goal`, `date`, `go_out`, `career_c`
---
"""

# the categorical columns
cat_col = ['field_cd', 'race','race_o', 'goal', 'date', 'go_out', 'career_c']

def one_hot_categroy_column(df, cte_col, name):
    # replace the nan value with the -1
    df[cte_col] = imputed_df[cte_col].apply(lambda x: -1 if x =='nan' else float(x))
    # group by the participant and get the correponding column information
    one_hot_df = pd.get_dummies(imputed_df[['iid', cte_col]].groupby('iid')[cte_col].agg(lambda x: pd.Series.mode(x)[0]))
    # replace the column name of numeric code with code
    code_col = [name + str(int(code)) for code in list(one_hot_df.columns)]
    one_hot_df.columns = code_col
    return one_hot_df

# career_dictionary
career_dict = {'cr_1': 'Lawyer', 'cr_2': 'Academic/Research', 'cr_3': 'Psychologist',
               'cr_4': 'Doctor/Medicine', 'cr_5': 'Engineer',
               'cr_6': 'Creative Arts/Entertainment',
               'cr_7': 'Banking/Consulting/Finance/Marketing/Business/CEO/Entrepreneur/Admin',
               'cr_8': 'Real Estate',
               'cr_9': 'International/Humanitarian Affairs ',
               'cr_10': 'Undecided', 'cr_11': 'Social Work',
               'cr_12': 'Speech Pathology', 'cr_13': 'Politics','cr_14': 'Pro sports/Athletics',
               'cr_15': 'Other','cr_16': 'Journalism', 'cr_17': 'Architecture', 'cr_-1': 'Not Answered'
              }

# field dictionary
field_dict =  {'fd_1': 'Law', 'fd_2': 'Math', 'fd_3': 'Social Science, Psychologist ',
               'fd_4': 'Medical Science, Pharmaceuticals, and Bio Tech ', 'fd_5': 'Engineering',
               'fd_6': 'English/Creative Writing/ Journalism', 'fd_7': 'History/Religion/Philosophy',
               'fd_8': 'Business/Econ/Finance', 'fd_9': 'Education, Academia',
               'fd_10': 'Biological Sciences/Chemistry/Physics', 'fd_11': 'Social Work',
               'fd_12': 'Undergrad/undecided', 'fd_13': 'Political Science/International Affairs',
               'fd_14': 'Film', 'fd_15': 'Fine Arts/Arts Administration',
               'fd_16': 'Languages', 'fd_17': 'Languages',
               'fd_18': 'Other', 'fd_-1': 'Not Answered'
              }

# race dictionary
race_dict =  {'race_1': 'Black/African American',
              'race_2': 'European/Caucasian-American',
              'race_3': 'Latino/Hispanic American',
              'race_4': 'Latino/Hispanic American',
              'race_5': 'Native American',
              'race_6': 'Other',
              'race_-1': 'Not Answered'
              }

# goal dictionary
goal_dict =  {'goal_1': 'Seemed like a fun night out',
              'goal_2': 'To meet new people',
              'goal_3': 'To get a date',
              'goal_4': 'Looking for a serious relationship',
              'goal_5': 'To say I did it',
              'goal_6': 'Other',
              'goal_-1': 'Not Answered'
              }

# date dictionary
# TODO: list the date code representation
date_dict =  {'date_1': 'Serveral times a week',
              'date_2': 'Twice a week',
              'date_3': 'Once a week',
              'date_4': 'Twice a month',
              'date_5': 'Once a month',
              'date_6': 'Several times a year',
              'date_7': 'Almost never'
              }

# go out dictionary
# TODO: list the go out code representation
go_out_dict = {'go_out_1': 'Serveral times a week',
               'go_out_2': 'Twice a week',
               'go_out_3': 'Once a week',
               'go_out_4': 'Twice a month',
               'go_out_5': 'Once a month',
               'go_out_6': 'Several times a year',
               'go_out_7': 'Almost never'
              }

# one hot encode the race column
one_hot_race_df = one_hot_categroy_column(imputed_df, 'race', 'race')
# one hot encode the race column
one_hot_race_o_df = one_hot_categroy_column(imputed_df, 'race_o', 'race_o_')
# one hot encode the career column
one_hot_career_df = one_hot_categroy_column(imputed_df, 'career_c', 'cr_')
# one hot encode the study field column
one_hot_field_df = one_hot_categroy_column(imputed_df, 'field_cd', 'fd_')
# one hot encode the goal column
one_hot_goal_df = one_hot_categroy_column(imputed_df, 'goal', 'goal_')
# one hot encode the date column
one_hot_date_df = one_hot_categroy_column(imputed_df, 'date', 'date_')
# one hot encode the go_out column
one_hot_out_df = one_hot_categroy_column(imputed_df, 'go_out', 'out_')

one_hot_race_o_df

# combine all the one_hot_category dataframe
one_hot_cate_all_df = pd.concat([one_hot_race_o_df, one_hot_race_df, one_hot_career_df, one_hot_field_df,
                                 one_hot_goal_df, one_hot_out_df, one_hot_date_df], axis=1)

# drop the columns after finish feature engineering?
imputed_df.drop(columns = cat_col, axis = 1, inplace = True)

"""---
### Extract the value for consistent numerical values
- for some numerical features, each participant keeps the same value during the entire experiment since they represent their own personality/hobby/characteristics. For these features, we extract them as the representative value of each participant.
- for the self-evaluation before the experiment, the scores are consistent for each participant, we handle them with the same way as other consistent numerical values.
- Those numerical columns are the following:
    - ['gender','imprace', 'imprelig','income', 'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art',
       'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies','concerts', 'music', 'shopping', 'yoga']
    - ['attr1_1', 'sinc1_1','intel1_1', 'fun1_1', 'amb1_1', 'shar1_1','attr4_1', 'sinc4_1','intel4_1',
       'fun4_1', 'amb4_1', 'shar4_1', 'attr2_1', 'sinc2_1','intel2_1', 'fun2_1', 'amb2_1', 'shar2_1',
       'attr3_1', 'sinc3_1','fun3_1', 'intel3_1', 'amb3_1', 'attr5_1', 'sinc5_1', 'intel5_1','fun5_1', 'amb5_1']
---
"""

def numerical_median_column(df, num_col, name = False):
    return df.groupby('iid')[num_col].median().to_frame()

# all the numerical columns with consistent values of one participant
consis_num_cols = ['gender','imprace', 'imprelig', 'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art',
                   'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies','concerts', 'music', 'shopping',
                   'yoga', 'attr1_1', 'sinc1_1','intel1_1', 'fun1_1', 'amb1_1', 'shar1_1','attr4_1', 'sinc4_1','intel4_1',
                   'fun4_1', 'amb4_1', 'shar4_1', 'attr2_1', 'sinc2_1','intel2_1', 'fun2_1', 'amb2_1', 'shar2_1',
                   'attr3_1', 'sinc3_1','fun3_1', 'intel3_1', 'amb3_1', 'attr5_1', 'sinc5_1', 'intel5_1','fun5_1', 'amb5_1'
                  ]
# generate the initial consisitent numerical features dataframe from the gender feature
consistent_numerical_df = numerical_median_column(imputed_df, consis_num_cols[0])
# for the rest of the consistent numerical features, append them to the previous gender dataframe
for num_col in consis_num_cols[1:]:
    temp_df = numerical_median_column(imputed_df, num_col)
    consistent_numerical_df = pd.concat([consistent_numerical_df, temp_df], axis=1)
# the consistent numerical engineered dataframe
consistent_numerical_df

# drop the columns after finish feature engineering?
imputed_df.drop(columns = consis_num_cols, axis = 1, inplace = True)

"""---
### Compute the Mean/Mode/Median for recurrent/inconsistent numerical values (self-evalution)
- for some numerical features, each participant possibly makes changes while dating different partners. For these features, we compute the mean/mode/median as the representative value of each participant.
- Those numerical columns are the following:
    - `prob`, `like`, `attr`, `sinc`,`intel`, `fun`, `amb`, `shar`, `like`, `prob`, `exphappy`, `samerace`
    - ['attr1_s','sinc1_s', 'intel1_s', 'fun1_s', 'amb1_s', 'shar1_s', 'attr3_s','sinc3_s', 'intel3_s', 'fun3_s', 'amb3_s']
    - `pf_o_att`, `attr_o`, `int_corr`, `sinc_o`, `intel_o`, `fun_o`, `amb_o`, `shar_o`, `like_o`, `prob_o`
 - did not find the definition of the features below, do the median/ mean first
    - `pf_o_sin`, `pf_o_int`, `pf_o_fun`, `pf_o_amb`, `pf_o_sha`
   
    
    -
---
"""

def numerical_mean_column(df, num_col):
    return df.groupby('iid')[num_col].mean().to_frame()

# all the numerical columns with inconsistent values
inconsis_num_cols = ['match_es','attr', 'sinc','intel', 'fun', 'amb', 'shar', 'like', 'prob', 'exphappy','samerace',
                     'attr1_s','sinc1_s', 'intel1_s', 'fun1_s', 'amb1_s', 'shar1_s', 'attr3_s','sinc3_s','intel3_s',
                     'fun3_s', 'amb3_s', 'pf_o_att', 'attr_o', 'int_corr', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o',
                     'like_o', 'prob_o', 'pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb', 'pf_o_sha']
# generate the initial inconsisitent numerical features dataframe
inconsistent_numerical_df = numerical_mean_column(imputed_df, inconsis_num_cols[0])
# for the rest of the inconsistent numerical features, append them to the previous dataframe
for num_col in inconsis_num_cols[1:]:
    temp_df = numerical_mean_column(imputed_df, num_col)
    inconsistent_numerical_df = pd.concat([inconsistent_numerical_df, temp_df], axis=1)
# the inconsistent numerical engineered dataframe
inconsistent_numerical_df

# drop the columns after finish feature engineering?
imputed_df.drop(columns = inconsis_num_cols, axis = 1, inplace = True)

"""---
### Transform the Numerical Features to interval variables
- for some numerical features, it is more appropriate to transform into the interval information in the participant level.
- for interval transformation, the function defines what each interval represents
- Those numerical columns are the following:
    - `age`, `age_o`, `mn_sat`, `income`
---
"""

# age interval transformation
def age_interval(age):
    if age < 20:
        return '<20'
    if 20 <= age < 25:
        return '20 - 24'
    if 25 <= age < 30:
        return '25-30'
    if age >= 30:
        return '>30'

# transform the age
age_df = imputed_df.groupby('iid')['age'].median().to_frame()
age_df['age'] = age_df['age'].apply(lambda x: age_interval(x))
age_df = pd.get_dummies(age_df)
age_df

# transform the age_o
age_o_df = imputed_df.groupby('iid')['age_o'].median().to_frame()
age_o_df['age_o'] = age_o_df['age_o'].apply(lambda x: age_interval(x))
age_o_df = pd.get_dummies(age_o_df)
age_o_df

# sat interval transformation
def sat_interval(sat):
    if sat <= 910:
        return 'Very Bad'
    if 910 < sat <= 1060:
        return 'Beyond Average'
    if 1060 <= sat < 1200:
        return 'Above Average'
    if sat >= 1200:
        return 'Very Good'

# sat interval transformation
sat_df = imputed_df.groupby('iid')['mn_sat'].median().to_frame()
sat_df['mn_sat'] = sat_df['mn_sat'].apply(lambda x: sat_interval(x))
sat_df.columns = ['SAT Condition']
sat_df = pd.get_dummies(sat_df)
sat_df

# income interval transformation
def income_interval(income):
    if income <= 20000:
        return 'Low Income'
    if 20000 < income <= 30000:
        return 'Below Average Income'
    if 30000 < income <= 45000:
        return 'Average Income'
    if 45000 < income <= 60000:
        return 'Above Average Income'
    if income > 60000:
        return 'High Income'

# income interval transformation
income_df = imputed_df.groupby('iid')['income'].median().to_frame()
income_df['income'] = income_df['income'].apply(lambda x: income_interval(x))
income_df.columns = ['Income Condition']
income_df = pd.get_dummies(income_df)
income_df

# drop the columns after finish feature engineering?
imputed_df.drop(columns = ['age','age_o', 'mn_sat', 'income'], axis = 1, inplace = True)

# the match rate for each participant in the experiment
match_results_df = match_result_df.groupby('iid')['match'].mean().to_frame()

# combine all the feature engineered dataframes
featured_df = pd.concat([one_hot_cate_all_df, consistent_numerical_df, inconsistent_numerical_df,
                         age_df, age_o_df, sat_df, income_df, match_results_df,], axis=1)
featured_df

"""# Modeling

##### Now, we have imputed our dataset and changed the dataset from match level to participant level. We are ready to use such dataset to predict match rate for each participant with unique iid.

##### In this section, we are going to include:
1. Train Test Split
2. Models which includes PCA, Linear regression (base model)
3. Metric evaluation: MSE
5. Fairness Calculation
6. Bias mitigation
7. Comparision of metrics (fairness & mse) before and after mitigation
"""

from sklearn.preprocessing import scale
from sklearn import model_selection
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import plotly.express as px
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_selection import RFE
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from numpy import mean
from numpy import std
from numpy import absolute
from pandas import read_csv
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold

# plot y to see if it follows a normal distribution
a = featured_df['match']

# Creating histogram
# fig, ax = plt.subplots(figsize =(10, 7))
# ax.hist(a)
import scipy.stats as stats
stats.probplot(featured_df['match'], dist="norm", plot=plt)

# match is right skewed so we could use log to normalize it
# since 0 < = match rate <= 1, we transform y to ln(y+1)
featured_df['match'] = np.log(featured_df['match'] +1)

stats.probplot(featured_df['match'], dist="norm", plot=plt)

############################################### Train Test Split ##################################################
X = featured_df.iloc[:, :len(featured_df.columns)-1]
Y = featured_df['match']
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)

############################################### PCA + Linear Regression ##################################################
#scale the training and testing data
pca = PCA(n_components = 10)
X_reduced_train = pca.fit_transform(scale(X_train))
X_reduced_test = pca.transform(scale(X_test))[:,:1] # only select PCA1
#train PCR model on training data
pcr = LinearRegression()
pcr.fit(X_reduced_train[:,:1], Y_train)
# plot PCA to see explained_var_ratio and decide pca numbers
'''labels = {
    str(i): f"PC {i+1} ({var:.1f}%)"
    for i, var in enumerate(pca.explained_variance_ratio_ * 100)
}

fig = px.scatter_matrix(
    X_reduced_train,
    labels=labels,
    dimensions=range(4),
    color = Y_train
)
fig.update_traces(diagonal_visible=False)
fig.show()'''
# Determine explained variance using explained_variance_ration_ attribute
#
exp_var_pca = pca.explained_variance_ratio_
#
# Cumulative sum of eigenvalues; This will be used to create step plot
# for visualizing the variance explained by each principal component.
#
cum_sum_eigenvalues = np.cumsum(exp_var_pca)
#
# Create the visualization plot
#
plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')
plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.tight_layout()
plt.show()

#calculate RMSE
PCR_pred = pcr.predict(X_reduced_test)
pcr_test_mse = np.sqrt(mean_squared_error(Y_test, PCR_pred))
pcr_train_mse = np.sqrt(mean_squared_error(Y_train, pcr.predict(X_reduced_train[:,:1])))
print("PCR test MSE: " + str(pcr_test_mse))
print("PCR train MSE: " + str(pcr_train_mse))

############################################### Linear Regression (basic) ##################################################
lr_model = LinearRegression().fit(X_train, Y_train)
lr_y_test_pred = lr_model.predict(X_test)
lr_y_train_pred = lr_model.predict(X_train)
lr_test_mse = np.sqrt(mean_squared_error(Y_test, lr_y_test_pred))
lr_train_mse = np.sqrt(mean_squared_error(Y_train, lr_y_train_pred))
print("Linear Regression without CV test MSE: " + str(lr_test_mse))
print("Linear Regression without CV train MSE: " + str(lr_train_mse))

# define model evaluation method
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
# evaluate model
scores = cross_val_score(lr_model, X_train, Y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)
# force scores to be positive
scores = absolute(scores)
print('CV Mean MSE for linear regression: %.3f (%.3f)' % (mean(scores), std(scores)))

################################################ Ridge ##########################
# evaluate an ridge regression model on the dataset

# define model
ridge_model = Ridge(alpha=1.0)
# define model evaluation method
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
# evaluate model
scores = cross_val_score(ridge_model, X_train, Y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)
# force scores to be positive
scores = absolute(scores)
print('CV Mean MSE for Ridge Regression: %.3f (%.3f)' % (mean(scores), std(scores)))
ridge_model.fit(X_train, Y_train)
ridge_y_train_pred = ridge_model.predict(X_train)
ridge_y_test_pred = ridge_model.predict(X_test)
ridge_test_mse = np.sqrt(mean_squared_error(Y_test, ridge_y_test_pred))
ridge_train_mse = np.sqrt(mean_squared_error(Y_train, ridge_y_train_pred))
print("Ridge Regression without CV test MSE: " + str(ridge_test_mse))
print("Ridge Regression without CV train MSE: " + str(ridge_train_mse))

"""### Fairness Calculation & Bias Mitigation"""

def one_hot_gender_0(row):
  if row['gender'] == 0:
    return 1
  else:
    return 0

def one_hot_gender_1(row):
  if row['gender'] == 1:
    return 1
  else:
    return 0

# change X test gender to one hot encoding, so X_test has gender, gender_0, gender_1
X_test['gender_0'] = X_test.apply(lambda row: one_hot_gender_0(row), axis = 1)
X_test['gender_1'] = X_test.apply(lambda row: one_hot_gender_1(row), axis = 1)
cat_names_contains = ['race', 'cr_', 'fd_', 'goal_', 'out_', 'date_', 'gender_', 'age_', 'SAT', 'Income']
    #'field_cd', 'race','race_o', 'goal', 'date', 'go_out', 'career_c', 'age', 'income', 'mn_sat']

# get each category's group's row numbers
group_index = {}
Y_test = Y_test.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
# for each category, we find corresponding groups
for cat in cat_names_contains:
  groups = [gp for gp in featured_df.columns if gp.startswith(cat)]
  # race and gender are special cases, bc race_o contains race, but we don't want race_o(bc race_o is not about participant but partner)
  # gender are encoded ourselves above
  if cat == 'race':
    groups = [gp for gp in groups if len(gp) == 5]
  elif cat == 'gender_':
    groups = ['gender_0', 'gender_1']
    print(groups)
  groups_mse = []
  df_index_names = []
  # in each group, we find its corresponding indexes, predictions, actual values, and mse for different algorithm
  for group in groups:
    group_index[group] = list(X_test[X_test[group] == 1].index)
    Y_test_in_group = [Y_test[i] for i in group_index[group]]
    lr_pred_Y_in_group = [lr_y_test_pred[i] for i in group_index[group]] # add if more models like this line
    ridge_pred_Y_in_group = [ridge_y_test_pred[i] for i in group_index[group]]
    if len(Y_test_in_group) == 0:
      continue
    else:
      group_lr_mse = np.sqrt(mean_squared_error(Y_test_in_group, lr_pred_Y_in_group))
      group_ridge_mse = np.sqrt(mean_squared_error(Y_test_in_group, ridge_pred_Y_in_group))
      groups_mse.append([group_lr_mse, group_ridge_mse])
      df_index_names.append(group)
  df_groups_mse = pd.DataFrame(groups_mse, columns=['Linear Regression MSE', 'Ridge Regression MSE'], index = df_index_names)
  print(df_groups_mse)
  ax = df_groups_mse.plot.bar(rot=0)

"""### Takeaway
From diagrams above, we can see that bias exist in our algorithms towards race, career, fields, goal, go_out frequency, date, age, median SAT scores, and Income level, but, interestingly, not in gender.

In general, the bias/differences exist but not much in different category. However, with the plots above, we found that there is large difference in mse in date = 1 and other date frequency. Similary, SAT score has bias too.

##### Thus, we decided to explore by ploting actual match rate vs predicted match rate for different groups to visualize difference/bias in our algorithm.
"""

# plot predicted match rate vs actual match rate in different groups
for cat in cat_names_contains:
  fig, ax = plt.subplots()
  groups = [gp for gp in featured_df.columns if gp.startswith(cat)]
  # race and gender are special cases, bc race_o contains race, but we don't want race_o(bc race_o is not about participant but partner)
  # gender are encoded ourselves above
  if cat == 'race':
    groups = [gp for gp in groups if len(gp) == 5]
  elif cat == 'gender_':
    groups = ['gender_0', 'gender_1']
  groups_actual_pred = []
  df_pred_index_names = []
  # in each group, we find its corresponding indexes, predictions, actual values, and mse for different algorithm
  for group in groups:
    group_index[group] = list(X_test[X_test[group] == 1].index)
    Y_test_in_group = [Y_test[i] for i in group_index[group]]
    lr_pred_Y_in_group = [lr_y_test_pred[i] for i in group_index[group]] # add if more models like this line
    ridge_pred_Y_in_group = [ridge_y_test_pred[i] for i in group_index[group]]

    if len(Y_test_in_group) == 0:
      continue
    else:
      group_row = []
      for i in group_index[group]:
        groups_actual_pred.append([Y_test[i], lr_y_test_pred[i], group])
        group_row.append([Y_test[i], lr_y_test_pred[i], group])
      # ax = df_groups_prediction_lr.plot.scatter(x = 'Actual Values', y = 'Predicted Values')#, c = 'Groups', colormap='viridis')
      gp_df = pd.DataFrame(group_row, columns=['Actual Values', 'Predicted Values', 'Groups'])
      # ax.scatter(gp_df['Actual Values'], gp_df['Predicted Values'])
      plt.plot(gp_df['Actual Values'], gp_df['Predicted Values'], 'o')
      m, b = np.polyfit(gp_df['Actual Values'], gp_df['Predicted Values'], 1)
      # add linear regression line to scatterplot
      # plt.plot(gp_df['Actual Values'], m*gp_df['Actual Values']+b)
  plt.legend(groups)
  plt.show()
  #df_groups_prediction_lr = pd.DataFrame(groups_actual_pred, columns=['Actual Values', 'Predicted Values', 'Groups'])
  #df_groups_prediction_lr.plot(x='Actual Values', y='Predicted Values')

"""## Bias Mitigation
#### Evaluation
"""

import dalex as dx

pip install dalex

exp = dx.Explainer(lr_model, X_test.iloc[:,:len(X_test.columns)-2], Y_test, verbose=False)
exp.model_performance().result

protected_gender = np.where(X_test.gender == 0, 'majority_male', "female")
privileged_gender = 'female'

f_gender = exp.model_fairness(protected_gender, privileged_gender)
f_gender.fairness_check()

race_check = ['race1', 'race2', 'race3', 'race4', 'race6']
for race in race_check:
    print('-------------------Check the Race:' + race + '----------------------------')
    protected_race = np.where(X_test[race] == 1, 'majority', "other")
    privileged_race = 'other'
    f_race = exp.model_fairness(protected_race, privileged_race)
    f_race.fairness_check()
    print('--------------------------------------------------------------------')

X_test.race1

protected_race = np.where(X_test.race6 == 1, 'majority', "other")
privileged_race = 'other'

f_race = exp.model_fairness(protected_race, privileged_race)
f_race.fairness_check()

f_race.plot()

"""#### Race 6 Mitigation"""

# create a synthetic training set by duplicating X_train and Y_train
# after duplicating flip sex in the synthetic set
def flip_val(row, col):
   if row[col] == 0 :
     return 1
   return 0

X_train_sync_race6 = X_train.drop('race6', axis = 1)
X_train_sync_race6['race6'] = X_train.apply (lambda row: flip_val(row, 'race6'), axis=1)
Y_train_sync_race6 = Y_train
# augment the original training set by the synthetic set. In other words, concatenate them
X_train_aug_race6 = pd.concat((X_train, X_train_sync_race6))
Y_train_aug_race6 = pd.concat((Y_train, Y_train_sync_race6))

print(X_train_aug_race6.shape, Y_train_aug_race6.shape)

# define model
lr_model2 = LinearRegression()
# define model evaluation method
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
# evaluate model
# scores = cross_val_score(lr_model2, X_train_aug_race6, Y_train_aug_race6, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)
# force scores to be positive
# scores = absolute(scores)
# print('CV Mean MSE for Linear Regression after bias mitigation: %.3f (%.3f)' % (mean(scores), std(scores)))
lr_model2.fit(X_train_aug_race6, Y_train_aug_race6)
lr_miti_y_train_pred = lr_model2.predict(X_train_aug_race6)
lr_miti_y_test_pred = lr_model2.predict(X_test.iloc[:,:len(X_test.columns)-2])
lr_miti_test_mse = np.sqrt(mean_squared_error(Y_test, lr_miti_y_test_pred))
lr_miti_train_mse = np.sqrt(mean_squared_error(Y_train_aug_race6, lr_miti_y_train_pred))
print("Linear Regression after bias mitigation without CV test MSE: " + str(lr_miti_train_mse))
print("Linear Regression after bias mitigation without CV train MSE: " + str(lr_miti_test_mse))

# define model
ridge_model2 = Ridge(alpha = 1.0)
# define model evaluation method
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
# evaluate model
# scores = cross_val_score(lr_model2, X_train_aug_race6, Y_train_aug_race6, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)
# force scores to be positive
# scores = absolute(scores)
# print('CV Mean MSE for Linear Regression after bias mitigation: %.3f (%.3f)' % (mean(scores), std(scores)))
ridge_model2.fit(X_train_aug_race6, Y_train_aug_race6)
ridge_miti_y_train_pred = ridge_model2.predict(X_train_aug_race6)
ridge_miti_y_test_pred = ridge_model2.predict(X_test.iloc[:,:len(X_test.columns)-2])
ridge_miti_test_mse = np.sqrt(mean_squared_error(Y_test, lr_miti_y_test_pred))
ridge_miti_train_mse = np.sqrt(mean_squared_error(Y_train_aug_race6, lr_miti_y_train_pred))
print("Ridge Regression after bias mitigation without CV test MSE: " + str(ridge_miti_test_mse))
print("Ridge Regression after bias mitigation without CV train MSE: " + str(ridge_miti_train_mse))

"""#### check mse graph to see mse changes after bias mitigation"""

# get each category's group's row numbers
group_index = {}
races = ['race']
# for each category, we find corresponding groups
for cat in races:
  groups = [gp for gp in featured_df.columns if gp.startswith(cat)]
  # race and gender are special cases, bc race_o contains race, but we don't want race_o(bc race_o is not about participant but partner)
  # gender are encoded ourselves above
  if cat == 'race':
    groups = [gp for gp in groups if len(gp) == 5]
  groups_mse = []
  df_index_names = []
  # in each group, we find its corresponding indexes, predictions, actual values, and mse for different algorithm
  for group in groups:
    group_index[group] = list(X_test[X_test[group] == 1].index)
    Y_test_in_group = [Y_test[i] for i in group_index[group]]
    lr_pred_Y_in_group = [lr_y_test_pred[i] for i in group_index[group]] # add if more models like this line
    ridge_pred_Y_in_group = [ridge_y_test_pred[i] for i in group_index[group]]
    if len(Y_test_in_group) == 0:
      continue
    else:
      group_lr_mse = np.sqrt(mean_squared_error(Y_test_in_group, lr_pred_Y_in_group))
      group_ridge_mse = np.sqrt(mean_squared_error(Y_test_in_group, ridge_pred_Y_in_group))
      if group == 'race6':
        groups_mse.append([lr_miti_test_mse, ridge_miti_test_mse])
      else:
        groups_mse.append([group_lr_mse, group_ridge_mse])
      df_index_names.append(group)
  df_groups_mse = pd.DataFrame(groups_mse, columns=['Linear Regression MSE', 'Ridge Regression MSE'], index = df_index_names)
  print(df_groups_mse)
  ax = df_groups_mse.plot.bar(rot=0)

"""##### fairness metric check (could be wrong)"""



exp = dx.Explainer(lr_model2, X_test.iloc[:,:len(X_test.columns)-2], Y_test, verbose=False)
exp.model_performance().result

protected_race = np.where(X_test.race6 == 1, 'majority', "other")
privileged_race = 'other'

f_race = exp.model_fairness(protected_race, privileged_race)
f_race.fairness_check()



"""# Feature Engineering for RACE, RACE_O"""

group_info_col = ['wave', 'round', 'order', 'pid']
useless_col = ['tuition']
# records the match results for each participant with its partenr information
match_result_df = imputed_df[['iid', 'race_o', 'match']]

# drop unnecessary features
imputed_df = imputed_df.drop(columns = group_info_col + useless_col + ['match'])

# the categorical columns
cat_col = ['field_cd', 'race', 'goal', 'date', 'go_out', 'career_c']

def one_hot_categroy_column(df, cte_col, name):
    # replace the nan value with the -1
    df[cte_col] = imputed_df[cte_col].apply(lambda x: -1.0 if x =='nan' else float(x))
    # group by the participant and get the correponding column information
    if name != 'race_o_':
      one_hot_df = pd.get_dummies(imputed_df[['iid', 'race_o', cte_col]].groupby(['iid','race_o'])[cte_col].agg(lambda x: pd.Series.mode(x)[0]))
    else:
      one_hot_df = pd.get_dummies(imputed_df[['iid', 'race_o']].groupby(['iid','race_o'])[cte_col].agg(lambda x: pd.Series.mode(x)[0]))
    # replace the column name of numeric code with code
    code_col = [name + str(int(code)) for code in list(one_hot_df.columns)]
    one_hot_df.columns = code_col
    return one_hot_df

# one hot encode the race column
one_hot_race_df = one_hot_categroy_column(imputed_df, 'race', 'race')
# one hot encode the race_o column
#one_hot_race_o_df = one_hot_categroy_column(imputed_df, 'race_o', 'race_o_')
# one hot encode the career column
one_hot_career_df = one_hot_categroy_column(imputed_df, 'career_c', 'cr_')
# one hot encode the study field column
one_hot_field_df = one_hot_categroy_column(imputed_df, 'field_cd', 'fd_')
# one hot encode the goal column
one_hot_goal_df = one_hot_categroy_column(imputed_df, 'goal', 'goal_')
# one hot encode the date column
one_hot_date_df = one_hot_categroy_column(imputed_df, 'date', 'date_')
# one hot encode the go_out column
one_hot_out_df = one_hot_categroy_column(imputed_df, 'go_out', 'out_')

# combine all the one_hot_category dataframe
one_hot_cate_all_df = pd.concat([one_hot_race_df, one_hot_career_df, one_hot_field_df,
                                 one_hot_goal_df, one_hot_out_df, one_hot_date_df], axis=1)

# drop the columns after finish feature engineering?
imputed_df.drop(columns = cat_col, axis = 1, inplace = True)

def numerical_median_column(df, num_col, name = False):
    return df.groupby(['iid','race_o'])[num_col].median().to_frame()

# all the numerical columns with consistent values of one participant
consis_num_cols = ['gender','imprace', 'imprelig', 'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art',
                   'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies','concerts', 'music', 'shopping',
                   'yoga', 'attr1_1', 'sinc1_1','intel1_1', 'fun1_1', 'amb1_1', 'shar1_1','attr4_1', 'sinc4_1','intel4_1',
                   'fun4_1', 'amb4_1', 'shar4_1', 'attr2_1', 'sinc2_1','intel2_1', 'fun2_1', 'amb2_1', 'shar2_1',
                   'attr3_1', 'sinc3_1','fun3_1', 'intel3_1', 'amb3_1', 'attr5_1', 'sinc5_1', 'intel5_1','fun5_1', 'amb5_1','mn_sat'
                  ]
# generate the initial consisitent numerical features dataframe from the gender feature
consistent_numerical_df = numerical_median_column(imputed_df, consis_num_cols[0])
# for the rest of the consistent numerical features, append them to the previous gender dataframe
for num_col in consis_num_cols[1:]:
    temp_df = numerical_median_column(imputed_df, num_col)
    consistent_numerical_df = pd.concat([consistent_numerical_df, temp_df], axis=1)
# the consistent numerical engineered dataframe
consistent_numerical_df

def numerical_mean_column(df, num_col):
    return df.groupby(['iid','race_o'])[num_col].mean().to_frame()

# all the numerical columns with inconsistent values
inconsis_num_cols = ['match_es','attr', 'sinc','intel', 'fun', 'amb', 'shar', 'like', 'prob', 'exphappy','samerace',
                     'attr1_s','sinc1_s', 'intel1_s', 'fun1_s', 'amb1_s', 'shar1_s', 'attr3_s','sinc3_s','intel3_s',
                     'fun3_s', 'amb3_s', 'pf_o_att', 'attr_o', 'int_corr', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o',
                     'like_o', 'prob_o', 'pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb', 'pf_o_sha']
# generate the initial inconsisitent numerical features dataframe
inconsistent_numerical_df = numerical_mean_column(imputed_df, inconsis_num_cols[0])
# for the rest of the inconsistent numerical features, append them to the previous dataframe
for num_col in inconsis_num_cols[1:]:
    temp_df = numerical_mean_column(imputed_df, num_col)
    inconsistent_numerical_df = pd.concat([inconsistent_numerical_df, temp_df], axis=1)
# the inconsistent numerical engineered dataframe
inconsistent_numerical_df

# drop the columns after finish feature engineering?
imputed_df.drop(columns = inconsis_num_cols, axis = 1, inplace = True)

# age interval transformation
def age_interval(age):
    if age < 20:
        return '<20'
    if 20 <= age < 25:
        return '20 - 24'
    if 25 <= age < 30:
        return '25-30'
    if age >= 30:
        return '>30'

# transform the age
age_df = imputed_df.groupby(['iid','race_o'])['age'].median().to_frame()
age_df['age'] = age_df['age'].apply(lambda x: age_interval(x))
age_df = pd.get_dummies(age_df)
age_df

# transform the age_o
age_o_df = imputed_df.groupby(['iid','race_o'])['age_o'].median().to_frame()
age_o_df['age_o'] = age_o_df['age_o'].apply(lambda x: age_interval(x))
age_o_df = pd.get_dummies(age_o_df)
age_o_df

# income interval transformation
def income_interval(income):
    if income <= 20000:
        return 'Low Income'
    if 20000 < income <= 30000:
        return 'Below Average Income'
    if 30000 < income <= 45000:
        return 'Average Income'
    if 45000 < income <= 60000:
        return 'Above Average Income'
    if income > 60000:
        return 'High Income'

# income interval transformation
income_df = imputed_df.groupby(['iid','race_o'])['income'].median().to_frame()
income_df['income'] = income_df['income'].apply(lambda x: income_interval(x))
income_df.columns = ['Income Condition']
income_df = pd.get_dummies(income_df)
income_df

# drop the columns after finish feature engineering?
imputed_df.drop(columns = ['age','age_o', 'income'], axis = 1, inplace = True)

# the match rate for each participant in the experiment
match_results_df = match_result_df.groupby(['iid','race_o'])['match'].mean().to_frame()

# combine all the feature engineered dataframes
featured_df = pd.concat([one_hot_cate_all_df, consistent_numerical_df, inconsistent_numerical_df,
                         age_df, age_o_df, income_df, match_results_df,], axis=1)
featured_df

# match is right skewed so we could use log to normalize it
# since 0 < = match rate <= 1, we transform y to ln(y+1)
featured_df['match'] = np.log(featured_df['match'] +1)

from sklearn.preprocessing import scale
from sklearn import model_selection
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import plotly.express as px
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.feature_selection import RFE
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from numpy import mean
from numpy import std
from numpy import absolute
from pandas import read_csv
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold

############################################### Train Test Split ##################################################
featured_df.reset_index(level='race_o', inplace = True)
race_o_df = pd.get_dummies(featured_df.race_o, prefix = 'race_o')
final_df = pd.concat([race_o_df,featured_df], axis =1)
final_df.drop('race_o', axis = 1, inplace = True)
X = final_df.iloc[:, :len(featured_df.columns)-1]
Y = final_df['match']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)

# corr of  all independent var to see correlation(colinearity)
df_corr = X_train.corr().abs()

# Select upper triangle of correlation matrix
upper = df_corr.where(np.triu(np.ones(df_corr.shape), k=1).astype(bool))

# Find features with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]

# Drop columns which have high corr
X_train.drop(to_drop, axis=1, inplace=True)
X_test.drop(to_drop, axis=1, inplace=True)

# p value for whole data
import statsmodels.api as sm
est = sm.OLS(Y_train, X_train)
est2 = est.fit()
print(est2.summary())

"""### Race 1 bias"""

# test for p value in race1
train = pd.concat([X_train, Y_train], axis = 1)
train_race1 = train[(train['race1'] == 1)]
X_train_race1 = train_race1.iloc[:,:-1]
Y_train_race1 = train_race1.iloc[:,-1]
import statsmodels.api as sm
est = sm.OLS(Y_train_race1, X_train_race1)
est2 = est.fit()
for col in X_train_race1.columns.tolist():
  if est2.pvalues[col] <= 0.05:
    print(col + " p-value: " + str(est2.pvalues[col] ))
'''
for p in est2.summary2().tables[1]['P>|t|']:
  if p <= 0.05:
    print(p)
#print(est2.summary2().tables[1]['P>|t|'])
'''

# sincerity vs match
# create a test table
import math
legends = []
for race_o in range(5):
  # fig, ax = plt.subplots()
  test_df =[]
  sinc = [i for i in range(17)]
  for score in sinc:
    row = [0 for i in range(len(X_train.columns))]
    row[84] = score
    row[107] = 1400  #sat score
    row[race_o] = 1 # race_o = 1
    row[6] = 1 # race = 1
    row[11] = 1 # cr = 1
    row[28] = 1 # fd = 2
    row[44] = 1 # goal = 1
    row[50] = 1 # out =1
    row[57] = 1 # date = 1
    row[64] =1 # gender = 1
    row[141] = 1 # age 20-24
    row[-1] = 1 # income
    for num in range(65,141): # for col no categorical, use median
      if num == 107 or num == 84:
        continue
      row[num] = math.floor(X_train_race1.iloc[:, num].median())
    test_df.append(row)
  test_df = pd.DataFrame(np.array(test_df), columns=X_train_race1.columns)
  lr_model = LinearRegression().fit(X_train_race1, Y_train_race1)
  lr_match_pred = lr_model.predict(test_df)
  plt.plot(test_df['sinc1_1'],[math.exp(x)-1 for x in lr_match_pred],linewidth=2, marker ='.') # change to original match rate
  # plt.plot(test_df['sinc1_1'],lr_match_pred,linewidth=2, marker ='.')
  plt.xlabel('Expected Sincerity from Male')
  plt.ylabel('Predicted Match Rate')
  if race_o + 1 == 5:
    legends.append('race = 1 and race_o = '+str(6))
  else:
    legends.append('race = 1 and race_o = '+str(race_o+1))
plt.legend(legends)
plt.show()

X_train.columns[130:]

# attr_o vs match rate
# create a test table
import math
legends = []
for race_o in range(5):
  # fig, ax = plt.subplots()
  test_df =[]
  sinc = [i for i in range(10, 20)]
  for score in sinc:
    row = [0 for i in range(len(X_train.columns))]
    row[129] = score
    row[107] = 1400  #sat score
    row[race_o] = 1 # race_o = 1
    row[6] = 1 # race = 1
    row[11] = 1 # cr = 1
    row[28] = 1 # fd = 2
    row[44] = 1 # goal = 1
    row[50] = 1 # out =1
    row[57] = 1 # date = 1
    row[64] =1 # gender = 1
    row[141] = 1 # age 20-24
    row[-1] = 1 # income
    for num in range(65,141): # for col no categorical, use median
      if num == 107 or num == 129:
        continue
      row[num] = math.floor(X_train_race1.iloc[:, num].median())
    test_df.append(row)
  test_df = pd.DataFrame(np.array(test_df), columns=X_train_race1.columns)
  lr_model = LinearRegression().fit(X_train_race1, Y_train_race1)
  lr_match_pred = lr_model.predict(test_df)
  plt.plot(test_df['attr_o'],[math.exp(x)-1 for x in lr_match_pred],linewidth=2, marker ='.') # change to original match rate
  plt.xlabel('Rating of Attractiveness by Female')
  plt.ylabel('Predicted Match Rate')
  if race_o + 1 == 5:
    legends.append('race = 1 and race_o = '+str(6))
  else:
    legends.append('race = 1 and race_o = '+str(race_o+1))
plt.legend(legends)
plt.show()

# pf_sin_o vs match rate
# create a test table
import math
legends = []
for race_o in range(5):
  # fig, ax = plt.subplots()
  test_df =[]
  sinc = [i for i in range(20)]
  for score in sinc:
    row = [0 for i in range(len(X_train.columns))]
    row[136] = score
    row[107] = 1400  #sat score
    row[race_o] = 1 # race_o = 1
    row[6] = 1 # race = 1
    row[11] = 1 # cr = 1
    row[28] = 1 # fd = 2
    row[44] = 1 # goal = 1
    row[50] = 1 # out =1
    row[57] = 1 # date = 1
    row[64] =1 # gender = 1
    row[141] = 1 # age 20-24
    row[-1] = 1 # income
    for num in range(65,141): # for col no categorical, use median
      if num == 107 or num == 136:
        continue
      row[num] = math.floor(X_train_race1.iloc[:, num].median())
    test_df.append(row)
  test_df = pd.DataFrame(np.array(test_df), columns=X_train_race1.columns)
  lr_model = LinearRegression().fit(X_train_race1, Y_train_race1)
  lr_match_pred = lr_model.predict(test_df)
  plt.plot(test_df['pf_o_sin'], [math.exp(x)-1 for x in lr_match_pred],linewidth=2, marker ='.') # change to original match rate
  plt.xlabel('Expected Sincerity from Female')
  plt.ylabel('Predicted ln(Match Rate + 1)')
  if race_o + 1 == 5:
    legends.append('race = 1 and race_o = '+str(6))
  else:
    legends.append('race = 1 and race_o = '+str(race_o+1))
plt.legend(legends)
plt.show()

# pf_o_amb vs match rate
# create a test table
import math
legends = []
for race_o in range(5):
  # fig, ax = plt.subplots()
  test_df =[]
  sinc = [i for i in range(100)]
  for score in sinc:
    row = [0 for i in range(len(X_train.columns))]
    row[139] = score
    row[107] = 1400  #sat score
    row[race_o] = 1 # race_o = 1
    row[6] = 1 # race = 1
    row[11] = 1 # cr = 1
    row[28] = 1 # fd = 2
    row[44] = 1 # goal = 1
    row[50] = 1 # out =1
    row[57] = 1 # date = 1
    row[64] =1 # gender = 1
    row[141] = 1 # age 20-24
    row[-1] = 1 # income
    for num in range(65,141): # for col no categorical, use median
      if num == 107 or num == 139:
        continue
      row[num] = math.floor(X_train_race1.iloc[:, num].median())
    test_df.append(row)
  test_df = pd.DataFrame(np.array(test_df), columns=X_train_race1.columns)
  lr_model = LinearRegression().fit(X_train_race1, Y_train_race1)
  lr_match_pred = lr_model.predict(test_df)
  plt.plot(test_df['pf_o_amb'],[math.exp(x)-1 for x in lr_match_pred],linewidth=2, marker ='.') # change to original match rate
  plt.xlabel('Expected Ambitiousness from Female')
  plt.ylabel('Predicted Match Rate')
  if race_o + 1 == 5:
    legends.append('race = 1 and race_o = '+str(6))
  else:
    legends.append('race = 1 and race_o = '+str(race_o+1))
plt.legend(legends)
plt.show()

"""#### Race2 bias"""

# test for p value in race1
train = pd.concat([X_train, Y_train], axis = 1)
train_race1 = train[(train['race2'] == 1)]
X_train_race1 = train_race1.iloc[:,:-1]
Y_train_race1 = train_race1.iloc[:,-1]
import statsmodels.api as sm
est = sm.OLS(Y_train_race1, X_train_race1)
est2 = est.fit()
for col in X_train_race1.columns.tolist():
  if est2.pvalues[col] <= 0.05:
    print(col + " p-value: " + str(est2.pvalues[col] ))

# attr_o vs match rate
# create a test table
import math
legends = []
for race_o in range(5):
  # fig, ax = plt.subplots()
  test_df =[]
  sinc = [i for i in range(10,20)]
  for score in sinc:
    row = [0 for i in range(len(X_train.columns))]
    row[129] = score
    row[107] = 1400  #sat score
    row[race_o] = 1 # race_o = 1
    #row[3] =1
    row[7] = 1 # race = 1
    row[11] = 1 # cr = 1
    row[28] = 1 # fd = 2
    row[44] = 1 # goal = 1
    row[50] = 1 # out =1
    row[57] = 1 # date = 1
    row[64] =1 # gender = 1
    row[141] = 1 # age 20-24
    row[-1] = 1 # income
    for num in range(65,141): # for col no categorical, use median
      if num == 107 or num == 129:
        continue
      row[num] = math.floor(X_train_race1.iloc[:, num].median())
    test_df.append(row)
  test_df = pd.DataFrame(np.array(test_df), columns=X_train_race1.columns)
  lr_model = LinearRegression().fit(X_train_race1, Y_train_race1)
  lr_match_pred = lr_model.predict(test_df)
  plt.plot(test_df['attr_o'],[math.exp(x)-1 for x in lr_match_pred],linewidth=2, marker ='.') # change to original match rate
  plt.xlabel('Rating of Attractiveness by Female')
  plt.ylabel('Predicted Match Rate')
  if race_o + 1 == 5:
    legends.append('race = 2 and race_o = '+str(6))
  else:
    legends.append('race = 2 and race_o = '+str(race_o+1))
plt.legend(legends)
plt.show()

"""#### Race 3 bias"""

# test for p value in race3
train = pd.concat([X_train, Y_train], axis = 1)
train_race1 = train[(train['race3'] == 1)]
X_train_race1 = train_race1.iloc[:,:-1]
Y_train_race1 = train_race1.iloc[:,-1]
import statsmodels.api as sm
est = sm.OLS(Y_train_race1, X_train_race1)
est2 = est.fit()
for col in X_train_race1.columns.tolist():
  if est2.pvalues[col] <= 0.05:
    print(col + " p-value: " + str(est2.pvalues[col] ))

# no significant factors

"""#### Race4 bias"""

# test for p value in race4
train = pd.concat([X_train, Y_train], axis = 1)
train_race1 = train[(train['race4'] == 1)]
X_train_race1 = train_race1.iloc[:,:-1]
Y_train_race1 = train_race1.iloc[:,-1]
import statsmodels.api as sm
est = sm.OLS(Y_train_race1, X_train_race1)
est2 = est.fit()
for col in X_train_race1.columns.tolist():
  if est2.pvalues[col] <= 0.05:
    print(col + " p-value: " + str(est2.pvalues[col] ))

# attr_o vs match rate
# create a test table
import math
legends = []
for race_o in range(5):
  # fig, ax = plt.subplots()
  test_df =[]
  sinc = [i for i in range(10,20)]
  for score in sinc:
    row = [0 for i in range(len(X_train.columns))]
    row[129] = score
    row[107] = 1400  #sat score
    row[race_o] = 1 # race_o = 1
    #row[3] =1
    row[9] = 1 # race = 1
    row[11] = 1 # cr = 1
    row[28] = 1 # fd = 2
    row[44] = 1 # goal = 1
    row[50] = 1 # out =1
    row[57] = 1 # date = 1
    row[64] =1 # gender = 1
    row[141] = 1 # age 20-24
    row[-1] = 1 # income
    for num in range(65,141): # for col no categorical, use median
      if num == 107 or num == 129:
        continue
      row[num] = math.floor(X_train_race1.iloc[:, num].median())
    test_df.append(row)
  test_df = pd.DataFrame(np.array(test_df), columns=X_train_race1.columns)
  lr_model = LinearRegression().fit(X_train_race1, Y_train_race1)
  lr_match_pred = lr_model.predict(test_df)
  plt.plot(test_df['attr_o'],[math.exp(x)-1 for x in lr_match_pred],linewidth=2, marker ='.') # change to original match rate
  plt.xlabel('Rating of Attractiveness by Female')
  plt.ylabel('Predicted Match Rate')
  if race_o + 1 == 5:
    legends.append('race = 4 and race_o = '+str(6))
  else:
    legends.append('race = 4 and race_o = '+str(race_o+1))
plt.legend(legends)
plt.show()

"""#### Race6 Bias"""

# test for p value in race4
train = pd.concat([X_train, Y_train], axis = 1)
train_race1 = train[(train['race6'] == 1)]
X_train_race1 = train_race1.iloc[:,:-1]
Y_train_race1 = train_race1.iloc[:,-1]
import statsmodels.api as sm
est = sm.OLS(Y_train_race1, X_train_race1)
est2 = est.fit()
for col in X_train_race1.columns.tolist():
  if est2.pvalues[col] <= 0.05:
    print(col + " p-value: " + str(est2.pvalues[col] ))

# no significant factors

"""##### Linear Regression （bias in intelligence)"""

# create a test table
import math
legends = []
for race_o in range(5):
  # fig, ax = plt.subplots()
  test_df =[]
  sat = [i*100 for i in range(17)]
  for score in sat:
    row = [0 for i in range(len(X_train.columns))]
    row[107] = score
    row[race_o] = 1 # race_o = 1
    row[6] = 1 # race = 1
    row[11] = 1 # cr = 1
    row[28] = 1 # fd = 2
    row[44] = 1 # goal = 1
    row[50] = 1 # out =1
    row[57] = 1 # date = 1
    row[64] =1 # gender = 1
    row[141] = 1 # age 20-24
    row[-1] = 1 # income
    for num in range(65,141): # for col no categorical, use median
      if num == 107:
        continue
      row[num] = math.floor(X_train.iloc[:, num].median())
    test_df.append(row)
  test_df = pd.DataFrame(np.array(test_df), columns=X_train.columns)
  lr_model = LinearRegression().fit(X_train, Y_train)
  lr_match_pred = lr_model.predict(test_df)
  plt.plot(test_df['mn_sat'],[math.exp(x)-1 for x in lr_match_pred],linewidth=2, marker ='.') # change to original match rate
  plt.xlabel('Intelligence')
  plt.ylabel('Predicted Match Rate')
  if race_o + 1 == 5:
    legends.append('race = 1 and race_o = '+str(6))
  else:
    legends.append('race = 1 and race_o = '+str(race_o+1))
plt.legend(legends)
plt.show()

# create a test table
import math
legends = []
for race_o in range(5):
  # fig, ax = plt.subplots()
  test_df =[]
  sat = [i*100 for i in range(17)]
  for score in sat:
    row = [0 for i in range(len(X_train.columns))]
    row[107] = score
    row[race_o] = 1 # race_o = 1
    row[7] = 1 # race = 2
    row[11] = 1 # cr = 1
    row[28] = 1 # fd = 2
    row[44] = 1 # goal = 1
    row[50] = 1 # out =1
    row[57] = 1 # date = 1
    row[64] =1 # gender = 1
    row[141] = 1 # age 20-24
    row[-1] = 1 # income
    for num in range(65,141): # for col no categorical, use median
      if num == 107:
        continue
      row[num] = math.floor(X_train.iloc[:, num].median())
    test_df.append(row)
  test_df = pd.DataFrame(np.array(test_df), columns=X_train.columns)
  lr_model = LinearRegression().fit(X_train, Y_train)
  lr_match_pred = lr_model.predict(test_df)
  plt.plot(test_df['mn_sat'],[math.exp(x)-1 for x in lr_match_pred],linewidth=2, marker ='.') # change to original match rate
  plt.xlabel('Intelligence')
  plt.ylabel('Predicted Match Rate')
  if race_o + 1 == 5:
    legends.append('race = 2 and race_o = '+str(6))
  else:
    legends.append('race = 2 and race_o = '+str(race_o+1))
plt.legend(legends)
plt.show()

# create a test table
import math
legends = []
for race_o in range(5):
  # fig, ax = plt.subplots()
  test_df =[]
  sat = [i*100 for i in range(17)]
  for score in sat:
    row = [0 for i in range(len(X_train.columns))]
    row[107] = score
    row[race_o] = 1 # race_o = 1
    row[8] = 1 # race = 1
    row[11] = 1 # cr = 1
    row[28] = 1 # fd = 2
    row[44] = 1 # goal = 1
    row[50] = 1 # out =1
    row[57] = 1 # date = 1
    row[64] =1 # gender = 1
    row[141] = 1 # age 20-24
    row[-1] = 1 # income
    for num in range(65,141): # for col no categorical, use median
      if num == 107:
        continue
      row[num] = math.floor(X_train.iloc[:, num].median())
    test_df.append(row)
  test_df = pd.DataFrame(np.array(test_df), columns=X_train.columns)
  lr_model = LinearRegression().fit(X_train, Y_train)
  lr_match_pred = lr_model.predict(test_df)
  plt.plot(test_df['mn_sat'],[math.exp(x)-1 for x in lr_match_pred],linewidth=2, marker ='.') # change to original match rate
  plt.xlabel('Intelligence')
  plt.ylabel('Predicted Match Rate')
  if race_o + 1 == 5:
    legends.append('race = 3 and race_o = '+str(6))
  else:
    legends.append('race = 3 and race_o = '+str(race_o+1))
plt.legend(legends)
plt.show()

# create a test table
import math
legends = []
for race_o in range(5):
  # fig, ax = plt.subplots()
  test_df =[]
  sat = [i*100 for i in range(17)]
  for score in sat:
    row = [0 for i in range(len(X_train.columns))]
    row[107] = score
    row[race_o] = 1 # race_o = 1
    row[9] = 1 # race = 1
    row[11] = 1 # cr = 1
    row[28] = 1 # fd = 2
    row[44] = 1 # goal = 1
    row[50] = 1 # out =1
    row[57] = 1 # date = 1
    row[64] =1 # gender = 1
    row[141] = 1 # age 20-24
    row[-1] = 1 # income
    for num in range(65,141): # for col no categorical, use median
      if num == 107:
        continue
      row[num] = math.floor(X_train.iloc[:, num].median())
    test_df.append(row)
  test_df = pd.DataFrame(np.array(test_df), columns=X_train.columns)
  lr_model = LinearRegression().fit(X_train, Y_train)
  lr_match_pred = lr_model.predict(test_df)
  plt.plot(test_df['mn_sat'],[math.exp(x)-1 for x in lr_match_pred],linewidth=2, marker ='.') # change to original match rate
  plt.xlabel('Intelligence')
  plt.ylabel('Predicted Match Rate')
  if race_o + 1 == 5:
    legends.append('race = 4 and race_o = '+str(6))
  else:
    legends.append('race = 4 and race_o = '+str(race_o+1))
plt.legend(legends)
plt.show()

# create a test table
import math
legends = []
for race_o in range(5):
  # fig, ax = plt.subplots()
  test_df =[]
  sat = [i*100 for i in range(17)]
  for score in sat:
    row = [0 for i in range(len(X_train.columns))]
    row[107] = score
    row[race_o] = 1 # race_o = 1
    row[10] = 1 # race = 1
    row[11] = 1 # cr = 1
    row[28] = 1 # fd = 2
    row[44] = 1 # goal = 1
    row[50] = 1 # out =1
    row[57] = 1 # date = 1
    row[64] =1 # gender = 1
    row[141] = 1 # age 20-24
    row[-1] = 1 # income
    for num in range(65,141): # for col no categorical, use median
      if num == 107:
        continue
      row[num] = math.floor(X_train.iloc[:, num].median())
    test_df.append(row)
  test_df = pd.DataFrame(np.array(test_df), columns=X_train.columns)
  lr_model = LinearRegression().fit(X_train, Y_train)
  lr_match_pred = lr_model.predict(test_df)
  plt.plot(test_df['mn_sat'],[math.exp(x)-1 for x in lr_match_pred],linewidth=2, marker ='.') # change to original match rate
  plt.xlabel('Intelligence')
  plt.ylabel('Predicted Match Rate')
  if race_o + 1 == 5:
    legends.append('race = 6 and race_o = '+str(6))
  else:
    legends.append('race = 6 and race_o = '+str(race_o+1))
plt.legend(legends)
plt.show()

############################################### Linear Regression (basic) ##################################################
lr_model = LinearRegression().fit(X_train, Y_train)
lr_y_test_pred = lr_model.predict(X_test)
lr_y_train_pred = lr_model.predict(X_train)
lr_test_mse = np.sqrt(mean_squared_error(Y_test, lr_y_test_pred))
lr_train_mse = np.sqrt(mean_squared_error(Y_train, lr_y_train_pred))
print("Linear Regression without CV test MSE: " + str(lr_test_mse))
print("Linear Regression without CV train MSE: " + str(lr_train_mse))

############################################### PCA + Linear Regression ##################################################
#scale the training and testing data
pca = PCA(n_components = 10)
X_reduced_train = pca.fit_transform(scale(X_train))
X_reduced_test = pca.transform(scale(X_test))[:,:1] # only select PCA1
#train PCR model on training data
pcr = LinearRegression()
pcr.fit(X_reduced_train[:,:1], Y_train)
# plot PCA to see explained_var_ratio and decide pca numbers
'''labels = {
    str(i): f"PC {i+1} ({var:.1f}%)"
    for i, var in enumerate(pca.explained_variance_ratio_ * 100)
}

fig = px.scatter_matrix(
    X_reduced_train,
    labels=labels,
    dimensions=range(4),
    color = Y_train
)
fig.update_traces(diagonal_visible=False)
fig.show()'''
# Determine explained variance using explained_variance_ration_ attribute
#
exp_var_pca = pca.explained_variance_ratio_
#
# Cumulative sum of eigenvalues; This will be used to create step plot
# for visualizing the variance explained by each principal component.
#
cum_sum_eigenvalues = np.cumsum(exp_var_pca)
#
# Create the visualization plot
#
plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')
plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.tight_layout()
plt.show()

#calculate RMSE
PCR_pred = pcr.predict(X_reduced_test)
pcr_test_mse = np.sqrt(mean_squared_error(Y_test, PCR_pred))
pcr_train_mse = np.sqrt(mean_squared_error(Y_train, pcr.predict(X_reduced_train[:,:1])))
print("PCR test MSE: " + str(pcr_test_mse))
print("PCR train MSE: " + str(pcr_train_mse))

################################################ Ridge ##########################
# evaluate an ridge regression model on the dataset

# define model
ridge_model = Ridge(alpha=1.0)
# define model evaluation method
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
# evaluate model
scores = cross_val_score(ridge_model, X_train, Y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)
# force scores to be positive
scores = absolute(scores)
print('CV Mean MSE for Ridge Regression: %.3f (%.3f)' % (mean(scores), std(scores)))
ridge_model.fit(X_train, Y_train)
ridge_y_train_pred = ridge_model.predict(X_train)
ridge_y_test_pred = ridge_model.predict(X_test)
ridge_test_mse = np.sqrt(mean_squared_error(Y_test, ridge_y_test_pred))
ridge_train_mse = np.sqrt(mean_squared_error(Y_train, ridge_y_train_pred))
print("Ridge Regression without CV test MSE: " + str(ridge_test_mse))
print("Ridge Regression without CV train MSE: " + str(ridge_train_mse))

